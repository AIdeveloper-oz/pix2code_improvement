{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Code Improvement\n",
    "Please read the readme for more details the solutions and models. This notebook only gives some code to use them.\n",
    "\n",
    "To run the notebook, please first unpack the datasets/web/all_data.zip files. The final bypass model is also attached zipped in  model_instances/model_instances.zip for direct usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import shutil\n",
    "import math\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# SETUP  LOGGING\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logging.info(\"Log started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = Path('')\n",
    "pix2code_folder = main_folder/'pix2code_improvement'\n",
    "compiler_folder = main_folder/'compiler'\n",
    "original_data_folder = main_folder / 'datasets'\n",
    "model_instance_folder = main_folder / 'model_instances'\n",
    "\n",
    "training_data_folder_path = 'datasets/web/training_data'\n",
    "validation_data_folder_path ='datasets/web/validation_data'\n",
    "test_data_folder_path ='datasets/web/test_data'\n",
    "\n",
    "train_paths = list(glob.glob(os.path.join(training_data_folder_path, \"*_256.npz\")))\n",
    "valid_paths = list(glob.glob(os.path.join(validation_data_folder_path, \"*_256.npz\")))\n",
    "test_paths = list(glob.glob(os.path.join(test_data_folder_path, \"*_256.npz\")))\n",
    "image_transformer = ImageDataGenerator(horizontal_flip=True, width_shift_range=5, fill_mode='nearest', vertical_flip=True)\n",
    "\n",
    "\n",
    "sys.path.insert(1,str(pix2code_folder))\n",
    "from constants import (START_WORD, END_WORD, PLACEHOLDER, ORIGINAL_DATA_FOLDER_NAME, TRAINING_DATA_FOLDER_NAME, VALIDATION_DATA_FOLDER_NAME, \n",
    "                       TEST_DATA_FOLDER_NAME, CONTEXT_LENGTH, IMAGE_SIZE, DOMAIN_LANGUAGES)\n",
    "from eval_functions import eval_cnn_model, eval_code_error\n",
    "from vocabulary import load_voc_from_file\n",
    "from generators import dataset_generator_reader, features_only\n",
    "\n",
    "\n",
    "voc = load_voc_from_file('datasets/web/vocabulary.vocab')\n",
    "max_epochs=400\n",
    "image_size = 256\n",
    "batch_size =32\n",
    "code_max_length=100\n",
    "\n",
    "validation_ratio =0.1\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Preprocesses the data and produces .npz files containing the data for the samples in the train, validation and test folders. Required previous setup is that the dataset folder contains an 'web/all_data' folder containing the original image and code samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess_data\n",
    "preprocess_data(original_data_folder, domain_languages=['web'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show transformations\n",
    "This part is for the data augmentation, allowing the user to see what image transformations that are appropriat to use for the images when using the cnn/RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from utils import preprocess_image\n",
    "only_include_words = ['btn-green','btn-orange','btn-red']\n",
    "image_size = 256\n",
    "# Load the paths and generators\n",
    "training_data_folder_path = 'datasets/web/training_data'\n",
    "image_train_paths = list(glob.glob(os.path.join(training_data_folder_path, \"*.png\")))\n",
    "img_to_show = 5\n",
    "img_gen = ImageDataGenerator(horizontal_flip=True, width_shift_range=5, fill_mode='nearest', vertical_flip=True)\n",
    "fig, ax = plt.subplots(img_to_show, 3, figsize=(15, 5*img_to_show))\n",
    "for i in range(img_to_show):\n",
    "    img = preprocess_image(image_train_paths[i], IMAGE_SIZE)\n",
    "    img_trans1 = img_gen.random_transform(img)\n",
    "    img_trans2 = img_gen.random_transform(img)\n",
    "    ax[i,0].imshow(img)\n",
    "    _ = ax[i,0].set_title(\"Original\")\n",
    "    ax[i,1].imshow(img_trans1)\n",
    "    _ = ax[i,1].set_title(\"Trans1\")\n",
    "    ax[i,2].imshow(img_trans2)\n",
    "    _ = ax[i,2].set_title(\"Trans2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web CNN Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.pix2code_original_cnn_model import Pix2CodeOriginalCnnModel\n",
    "\n",
    "words_to_include = voc.object_words\n",
    "\n",
    "\n",
    "model_save_path = 'model_instances/pix2code_original_cnn_model.h5'\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=words_to_include, \n",
    "                                         image_transformer=image_transformer)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=words_to_include)\n",
    "model_instance = Pix2CodeOriginalCnnModel(words_to_include)\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile()\n",
    "\n",
    "if train_models:\n",
    "    # Prepare training\n",
    "    logger.info(\"Training model\")\n",
    "    training_steps = int(len(train_paths)/batch_size)*8\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                          validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                          epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "else:\n",
    "    logger.info(\"Loading existing model\")\n",
    "    model_instance.load_weights(str(model_save_path))\n",
    "# Eval\n",
    "train_errors, train_y, train_predictions = eval_cnn_model(model_instance, train_paths, voc, words_to_include,'train_accuracy')\n",
    "display(train_errors)\n",
    "validation_errors, validation_y, validation_predictions = eval_cnn_model(model_instance, valid_paths, voc, words_to_include,'validation_accuracy')\n",
    "display(validation_errors)\n",
    "test_errors, test_y, test_predictions = eval_cnn_model(model_instance, test_paths, voc, words_to_include,'test_accuracy')\n",
    "display(test_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top check which files gives error\n",
    "y_bad = validation_y[(validation_y!=validation_predictions.round()).apply(any,axis=1)]\n",
    "pred_bad = validation_predictions[(validation_y!=validation_predictions.round()).apply(any,axis=1)]\n",
    "print(\"Nr of incorrectly predicted files: {}\".format(y_bad.shape[0]))\n",
    "display(y_bad)\n",
    "display(pred_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.shallow_cnn_model import ShallowCnnModel\n",
    "\n",
    "words_to_include = list(voc.object_words)\n",
    "model_save_path = 'model_instances/shallow_cnn_model.h5'\n",
    "\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=words_to_include, \n",
    "                                         image_transformer=image_transformer)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=words_to_include)\n",
    "\n",
    "model_instance = ShallowCnnModel(words_to_include,dense_layer_size=512, dropout_ratio=0.1)\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile(optimizer=RMSprop(lr=0.0001, clipvalue=1.0), loss='mse')\n",
    "\n",
    "# Train\n",
    "if train_models:\n",
    "    logger.info(\"Training model\")\n",
    "    training_steps = int(len(train_paths)/batch_size)*8\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "else:\n",
    "    logger.info(\"Loading existing model\")\n",
    "    model_instance.load_weights(str(model_save_path))\n",
    "\n",
    "train_errors, train_y, train_predictions = eval_cnn_model(model_instance, train_paths, voc, words_to_include,'train_accuracy')\n",
    "display(train_errors)\n",
    "validation_errors, validation_y, validation_predictions = eval_cnn_model(model_instance, valid_paths, voc, words_to_include,'validation_accuracy')\n",
    "display(validation_errors)\n",
    "test_errors, test_y, test_predictions = eval_cnn_model(model_instance, test_paths, voc, words_to_include,'test_accuracy')\n",
    "display(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top check which files gives error\n",
    "y_bad = validation_y[(validation_y!=validation_predictions.round()).apply(any,axis=1)]\n",
    "pred_bad = validation_predictions[(validation_y!=validation_predictions.round()).apply(any,axis=1)]\n",
    "print(\"Nr of incorrectly predicted files: {}\".format(y_bad.shape[0]))\n",
    "display(y_bad)\n",
    "display(pred_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN layer output inspection (object detection and positioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = ShallowCnnModel(words_to_include,dense_layer_size=512, dropout_ratio=0.1, image_out=True)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile(optimizer=RMSprop(lr=0.00001, clipvalue=1.0), loss='mse')\n",
    "model_instance.load_weights(str(model_save_path))\n",
    "\n",
    "file_id = 'ADE9B442-C9D4-4583-BF96-374D792ACF19'\n",
    "npz_file = str(list(Path('datasets/web').rglob(file_id+\"_256.npz\"))[0])\n",
    "\n",
    "temp = np.load(npz_file, allow_pickle=True)\n",
    "img_data = temp['img_data']\n",
    "pred_all = model_instance.predict({'img_data': np.reshape(img_data, tuple([1]+list(img_data.shape)))})\n",
    "pred_all = {key:val[0] for key, val in pred_all.items()}\n",
    "n_plots = int(len(pred_all)/2)+1\n",
    "n_rows = math.ceil(n_plots/3)\n",
    "fig, ax = plt.subplots(n_rows,3,figsize=(15, 5*n_rows), squeeze=False)\n",
    "\n",
    "ax[0,0].imshow(cv2.cvtColor(img_data, cv2.COLOR_BGR2RGB),)\n",
    "for i, word in enumerate(voc.object_words, start=1):\n",
    "    if n_rows >1:\n",
    "        ax[int(i/3),i%3].imshow(pred_all['img_out_{}'.format(word)].squeeze(-1))\n",
    "        ax[int(i/3),i%3].set_title(\"{}:{}\".format(word,pred_all[word+\"_count\"][0]))    \n",
    "#plt.savefig('image_out_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web code model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model - but trained until validation error rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from models.pix2code_original_model import Pix2codeOriginalModel\n",
    "\n",
    "model_save_path =  'model_instances/pix2code_original_model.h5'\n",
    "\n",
    "# Build the model\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, \n",
    "                                         include_context_mode='single_word', include_code_mode='single_word',\n",
    "                                        fixed_output_length=CONTEXT_LENGTH)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, \n",
    "                                         include_context_mode='single_word', include_code_mode='single_word',\n",
    "                                        fixed_output_length=CONTEXT_LENGTH)\n",
    "model_instance = Pix2codeOriginalModel(voc.size)\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile(optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "if train_models:\n",
    "    logger.info(\"training_model\")\n",
    "    with open(training_data_folder_path + \"/nr_of_instances.txt\", 'r') as f:\n",
    "        nr_of_training_instances = int(f.read())\n",
    "    training_steps = int(nr_of_training_instances / batch_size)\n",
    "\n",
    "    with open(validation_data_folder_path + \"/nr_of_instances.txt\", 'r') as f:\n",
    "        nr_of_eval_instances = int(f.read())\n",
    "    val_steps = int(nr_of_eval_instances / batch_size)\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                             validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                             epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading weights\")\n",
    "        model_instance.load_weights(str(model_save_path))\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image model (Non-rnn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_save_path = 'model_instances/shallow_image_model.h5'\n",
    "from models.shallow_image_model import ShallowImageModel\n",
    "# need generators with x as img, y as {word +_count:count for each word + code:50 first onehotencoded}\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', fixed_output_length=code_max_length)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', fixed_output_length=code_max_length)\n",
    "\n",
    "model_instance = ShallowImageModel(voc.words,image_count_words=voc.object_words, dense_layer_size=512)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile()\n",
    "\n",
    "\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/shallow_cnn_model.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True)\n",
    "    \n",
    "    # Do initial training with transferred layers locked\n",
    "    for layer_name in ['cnn_unit','counter_unit']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = False\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    \n",
    "    # Unlock and continue training\n",
    "    for layer_name in ['cnn_unit','counter_unit']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])    \n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-image-model-repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.rnn_image_model_repeat import RnnImageModelRepeat\n",
    "model_save_path = 'model_instances/rnn_image_model_repeat.h5'\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "\n",
    "\n",
    "model_instance = RnnImageModelRepeat(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile()\n",
    "\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/shallow_image_model.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True)\n",
    "    # Do initial training with transferred layers locked\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = False\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=5\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps*5,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    # Unlock and continue training\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=10\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-image-model-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.rnn_image_model_memory import RnnImageModelMemory\n",
    "model_save_path = 'model_instances/rnn_image_model_memory.h5'\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "\n",
    "model_instance = RnnImageModelMemory(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "model_instance.compile()\n",
    "\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/shallow_image_model.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True)\n",
    "    # Do initial training with transferred layers locked\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = False\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=5\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps*5,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    # Unlock and continue training\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=10\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RNN TFA architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from models.rnn_image_model_tfa import RnnImageModelTfa\n",
    "model_save_path = 'model_instances/rnn_image_model_tfa.h5'\n",
    "\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "\n",
    "model_instance = RnnImageModelTfa(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "loss.update({'code':'categorical_crossentropy'})\n",
    "model_instance.compile(loss=loss,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/shallow_image_model.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True)\n",
    "    # Do initial training with transferred layers locked\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = False\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':100.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=5\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps*5,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    # Unlock and continue training\n",
    "    for layer_name in ['cnn_unit','counter_unit','ordering_1','ordering_2','ordering_3']:    \n",
    "        layer = model_instance.get_layer(layer_name)\n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN TFA - Repeat architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from models.rnn_image_model_tfa_repeat import RnnImageModelTfaRepeat\n",
    "model_save_path = 'model_instances/rnn_image_model_tfa_repeat.h5'\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length)\n",
    "\n",
    "model_instance = RnnImageModelTfaRepeat(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512, dropout_ratio=0.25)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "#model_instance.compile()\n",
    "loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "loss.update({'code':'categorical_crossentropy'})\n",
    "model_instance.compile(loss=loss,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=5\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=20\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN TFA - Repeat (bypass) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from models.rnn_image_model_tfa_repeat_bypass import RnnImageModelTfaRepeatBypass\n",
    "model_save_path = 'model_instances/rnn_image_model_tfa_repeat_bypass.h5'\n",
    "# need generators with x as img, y as {word +_count:count for each word + code:50 first onehotencoded}\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "\n",
    "model_instance = RnnImageModelTfaRepeatBypass(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512, dropout_ratio=0.25, order_layer_output_size=512, cell_type='lstm', \n",
    "                                              pool_using_strides=False)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "loss.update({'code':'categorical_crossentropy'})\n",
    "model_instance.compile(loss=loss,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "    \n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/rnn_image_model_tfa_repeat_bypass.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True, skip_mismatch=True)\n",
    "    for layer in model_instance.layers:    \n",
    "        if layer.name == 'ordering_final':\n",
    "            layer.trainable = True\n",
    "            logger.info(\"setting {} trainable\".format(layer.name))\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            logger.info(\"setting {} not trainable\".format(layer.name))\n",
    "\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps*2,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    for layer in model_instance.layers:        \n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN TFA - Repeat (bypass) architecture with strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models.rnn_image_model_tfa_repeat_bypass import RnnImageModelTfaRepeatBypass\n",
    "model_save_path = 'model_instances/rnn_image_model_tfa_repeat_bypass_strides.h5'\n",
    "# need generators with x as img, y as {word +_count:count for each word + code:50 first onehotencoded}\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "\n",
    "model_instance = RnnImageModelTfaRepeatBypass(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512, dropout_ratio=0.25, order_layer_output_size=512, pool_using_strides=True)\n",
    "# Build the model\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "loss.update({'code':'categorical_crossentropy'})\n",
    "model_instance.compile(loss=loss,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "if train_models:\n",
    "    training_steps = int(len(train_paths)/batch_size)*5\n",
    "    val_steps = int(len(valid_paths)/batch_size)\n",
    "\n",
    "    # Do the transfer learning\n",
    "    transfer_learning_model_save_path = 'model_instances/rnn_image_model_tfa_repeat_bypass.h5'\n",
    "    model_instance.load_weights(transfer_learning_model_save_path,by_name=True, skip_mismatch=True)\n",
    "    for layer in model_instance.layers:    \n",
    "        if \"_pool\" not in layer.name:\n",
    "            layer.trainable = True\n",
    "            logger.info(\"setting {} trainable\".format(layer.name))\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            logger.info(\"setting {} not trainable\".format(layer.name))\n",
    "\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':1.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    early_stopping_patience=5\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps*2,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    for layer in model_instance.layers:    \n",
    "        layer.trainable = True\n",
    "    loss = {word+\"_count\": 'mse' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    loss_weights = {word+\"_count\": 1/len( model_instance.image_count_words) for word in model_instance.image_count_words}\n",
    "    loss_weights.update({'code':10.0})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    model_instance.save_weights('model_instances/rnn_image_model_tfa_repeat_bypass_strides_memless_backup.h')\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "    loss = {word+\"_count\": 'mae' for word in model_instance.image_count_words}\n",
    "    loss.update({'code':'categorical_crossentropy'})\n",
    "    model_instance.compile(loss=loss, loss_weights=loss_weights,optimizer=RMSprop(lr=0.00001, clipvalue=1.0))\n",
    "    hist = model_instance.fit(train_dataset, steps_per_epoch=training_steps,\n",
    "                              validation_data=valid_dataset, validation_steps=val_steps,\n",
    "                              epochs=max_epochs, callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    train_df = eval_code_error(model_instance,train_paths,voc)\n",
    "    train_df.to_pickle(model_save_path[:-3]+\"_train_error.pickle\")    \n",
    "    valid_df = eval_code_error(model_instance,valid_paths,voc)\n",
    "    valid_df.to_pickle(model_save_path[:-3]+\"_valid_error.pickle\")    \n",
    "    test_df = eval_code_error(model_instance,test_paths,voc)\n",
    "    test_df.to_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "else:\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.info(\"Loading existing model\")\n",
    "        model_instance.load_weights(model_save_path)\n",
    "\n",
    "    train_df = pd.read_pickle(model_save_path[:-3]+\"_train_error.pickle\")\n",
    "    valid_df = pd.read_pickle(model_save_path[:-3]+\"_valid_error.pickle\")\n",
    "    test_df = pd.read_pickle(model_save_path[:-3]+\"_test_error.pickle\")\n",
    "for df, name in zip([train_df, valid_df,test_df],['train','valid','test']):    \n",
    "    logger.info(\"{0} error: {1:.4f}, correct: {2:.4}, length: {3:.4}, active: {4:.4}, but: {5:.4}\".format(\n",
    "        name, df.error.mean(), df.correctly_predicted.mean(),  df.same_length.mean(), df.active_button_correct.mean(), \n",
    "        df.button_color_correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot image output\n",
    "Example for tfa repat bypass with strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn_image_model_tfa_repeat_bypass import RnnImageModelTfaRepeatBypass\n",
    "model_save_path = 'model_instances/rnn_image_model_tfa_repeat_bypass_strides.h5'\n",
    "# need generators with x as img, y as {word +_count:count for each word + code:50 first onehotencoded}\n",
    "train_dataset = dataset_generator_reader(train_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code',include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "valid_dataset = dataset_generator_reader(valid_paths, voc, include_image_object_count_words=voc.object_words,\n",
    "                                         include_code_mode='full_code', include_context_mode='full_code', \n",
    "                                         fixed_output_length=code_max_length, batch_size=32)\n",
    "\n",
    "model_instance = RnnImageModelTfaRepeatBypass(voc.words,image_count_words=voc.object_words, max_code_length=code_max_length,\n",
    "                                     dense_layer_size=512, dropout_ratio=0.25, order_layer_output_size=512, pool_using_strides=True, image_out=True)\n",
    "pred = model_instance.predict(train_dataset.map(features_only).take(1))\n",
    "\n",
    "model_instance.compile(loss='mse',optimizer=RMSprop(lr=0.0001, clipvalue=1.0))\n",
    "if os.path.exists(model_save_path):\n",
    "    logger.info(\"Loading existing model\")\n",
    "    model_instance.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_code2context\n",
    "file_id = '1F4D3508-2479-4D8A-B5F0-92CF690BD1AE'\n",
    "path = str(list(Path('datasets/web/training_data').rglob(file_id+\"_256.npz\"))[0])\n",
    "print(path)\n",
    "temp = np.load(path, allow_pickle=True)\n",
    "context = preprocess_code2context(str(temp['code']), 'full_code', voc, 100, 0)\n",
    "pred = model_instance.predict({'img_data':np.expand_dims(temp['img_data'],0), 'context':np.expand_dims(context,0)})\n",
    "probas = pred['code'][0]\n",
    "prediction = \" \".join([voc.token2word_dict[val] for val in np.argmax(probas, axis=1)]).split(\" <eos>\")[0].replace(\" <pad>\", \"\")\n",
    "print(str(temp['code']).replace('close_square_bracket',\"]\").replace('square_bracket',\"[\"))\n",
    "print(prediction.replace('close_square_bracket',\"]\").replace('square_bracket',\"[\"))\n",
    "\n",
    "n_plots = int(len(voc.object_words))+1\n",
    "n_rows = math.ceil(n_plots/3)\n",
    "fig, ax = plt.subplots(n_rows,3,figsize=(15, 5*n_rows), squeeze=False)\n",
    "\n",
    "ax[0,0].imshow(cv2.cvtColor(temp['img_data'], cv2.COLOR_BGR2RGB),)\n",
    "for i in range(1, n_plots):\n",
    "    pred_img = pred[\"img_out_\"+voc.object_words[i-1]][0,:,:,0]\n",
    "    \n",
    "    ax[int(i/3),i%3].imshow(pred_img)\n",
    "    ax[int(i/3),i%3].set_title(\"{}:{}\".format(voc.object_words[i-1],pred[voc.object_words[i-1]+\"_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['pix2code_original_model','shallow_image_model','rnn_image_model_repeat','rnn_image_model_memory','rnn_image_model_tfa','rnn_image_model_tfa_repeat','rnn_image_model_tfa_repeat_bypass']:\n",
    "    ds_names = ['train','valid','test']\n",
    "    all_res_ls = []\n",
    "    #res = pd.DataFrame(index=ds_names, columns=['error','correctly_predicted'])\n",
    "    for ds_name in ds_names:\n",
    "        res_df = pd.read_pickle('model_instances/{}_{}_error.pickle'.format(model_name,ds_name))\n",
    "        all_res_ls.append(res_df[['error','correctly_predicted','same_length','active_button_correct','button_color_correct']].mean().rename(ds_name))\n",
    "\n",
    "    all_res_df = pd.concat(all_res_ls,axis=1).T \n",
    "    all_res_df.index.name='dataset'\n",
    "    print(model_name)\n",
    "    print(all_res_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for model_name in ['pix2code_original_model','shallow_image_model','rnn_image_model_repeat','rnn_image_model_memory','rnn_image_model_tfa','rnn_image_model_tfa_repeat','rnn_image_model_tfa_repeat_bypass']:\n",
    "    ds_names = ['train','valid','test']\n",
    "    all_res_ls = []\n",
    "    #res = pd.DataFrame(index=ds_names, columns=['error','correctly_predicted'])\n",
    "    for ds_name in ds_names:\n",
    "        res_df = pd.read_pickle('model_instances/{}_{}_error.pickle'.format(model_name,ds_name))\n",
    "        all_res_ls.append(res_df[['error','correctly_predicted','same_length','active_button_correct','button_color_correct']].mean().rename(ds_name))\n",
    "\n",
    "    all_res_df = pd.concat(all_res_ls,axis=1).T \n",
    "    all_res_df.index.name='dataset'\n",
    "    df_list.append(pd.concat([all_res_df], keys=[model_name],names=['model_name'] )[['error','correctly_predicted']])\n",
    "df = pd.concat(df_list).reset_index('dataset')\n",
    "df['Notes/advantages/disadvantages']=None\n",
    "print(df.to_markdown())    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
